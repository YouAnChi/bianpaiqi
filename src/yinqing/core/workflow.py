import os
import asyncio
import traceback
from typing import Dict, List, Any, Optional
from collections import deque
from yinqing.core.types import ExecutionPlan, TaskStep, ParallelConfig, generate_trace_id
from yinqing.core.parser import TaskParserLayer
from yinqing.core.matcher import CapabilityMatcherLayer
from yinqing.core.executor import TaskExecutorLayer
from yinqing.utils.logger import get_logger

logger = get_logger(__name__)

class WorkflowEngine:
    """é¡¹ç›®ç»ç†ï¼šæ”¯æŒå¹¶è¡Œæ‰§è¡Œå’Œä¾èµ–å¤„ç†çš„é€šç”¨ç¼–æ’å™¨"""
    
    def __init__(self):
        self.parser = TaskParserLayer()
        self.matcher = CapabilityMatcherLayer()
        self.executor = TaskExecutorLayer()
        self.global_context_store: Dict[str, Dict[str, Any]] = {}
        self.step_status_store: Dict[str, Dict[int, TaskStep]] = {}
        self.parallel_config = ParallelConfig(fail_strategy="continue", max_parallel=5)
        
        # Current context (runtime)
        self.global_context = {}
        
        # è¾“å‡ºç›®å½•
        self.output_dir = os.path.join(os.getcwd(), "output")
        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)

    def format_response(self, content: str, is_complete: bool = False):
        return {"content": content, "is_complete": is_complete}

    def _save_result_to_file(self, query: str, result: str, trace_id: str, plan: ExecutionPlan = None):
        """å°†æœ€ç»ˆç»“æœä¿å­˜åˆ°æ–‡ä»¶ï¼Œæ ¹æ®æ‰§è¡Œçš„Agentç±»å‹æ™ºèƒ½é€‰æ‹©æ ¼å¼"""
        try:
            # æ¸…ç†æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦
            safe_query = "".join([c for c in query if c.isalnum() or c in (' ', '-', '_')]).strip().replace(' ', '_')
            
            # æ£€æµ‹æ˜¯å¦æœ‰Excelæˆ–Wordç”Ÿæˆæ­¥éª¤
            has_excel = False
            has_word = False
            saved_files = []
            
            if plan:
                for step in plan.steps:
                    if step.status == "success" and step.assigned_agent:
                        agent_name = step.assigned_agent.name.lower()
                        if "excel" in agent_name:
                            has_excel = True
                        if "word" in agent_name:
                            has_word = True
            
            # é»˜è®¤ä¿å­˜Markdownæ–‡ä»¶
            md_filename = f"{safe_query}_{trace_id[:8]}.md"
            md_filepath = os.path.join(self.output_dir, md_filename)
            
            with open(md_filepath, "w", encoding="utf-8") as f:
                f.write(f"# Task: {query}\n\n")
                f.write(f"Trace ID: {trace_id}\n")
                f.write(f"Date: {asyncio.get_event_loop().time()}\n\n")
                f.write("---\n\n")
                f.write(result)
            
            saved_files.append(md_filepath)
            logger.info(f"âœ… Markdown result saved to {md_filepath}")
            
            # å¦‚æœæ£€æµ‹åˆ°Excelæˆ–Wordç”Ÿæˆï¼Œæ·»åŠ æç¤ºä¿¡æ¯
            if has_excel or has_word:
                file_types = []
                if has_excel:
                    file_types.append("Excel (.xlsx)")
                if has_word:
                    file_types.append("Word (.docx)")
                logger.info(f"ğŸ“Š æ£€æµ‹åˆ°ç”Ÿæˆäº† {', '.join(file_types)} æ–‡ä»¶ï¼Œè¯·æŸ¥çœ‹outputç›®å½•")
            
            return ", ".join(saved_files) if len(saved_files) > 1 else saved_files[0]
        except Exception as e:
            logger.error(f"Failed to save result to file: {e}")
            return None

    async def _execute_parallel_steps(self, steps: List[TaskStep], context: Dict[str, Any], trace_id: str):
        """å¹¶è¡Œæ‰§è¡Œå¤šä¸ªæ­¥éª¤ï¼Œè¿”å›æ‰§è¡Œç»“æœ"""
        # æ„å»ºå¹¶è¡Œä»»åŠ¡åˆ—è¡¨
        tasks = []
        for step in steps:
            if step.status == "pending":
                tasks.append(self.executor.execute_step(step, context, trace_id))
        
        if not tasks:
            return []

        # æ§åˆ¶æœ€å¤§å¹¶è¡Œæ•°
        if len(tasks) > self.parallel_config.max_parallel:
            chunks = [tasks[i:i+self.parallel_config.max_parallel] for i in range(0, len(tasks), self.parallel_config.max_parallel)]
            all_results = []
            for chunk in chunks:
                chunk_results = await asyncio.gather(*chunk, return_exceptions=self.parallel_config.fail_strategy == "continue")
                all_results.extend(chunk_results)
        else:
            # æ‰§è¡Œå¹¶è¡Œä»»åŠ¡ï¼Œreturn_exceptions=Trueè¡¨ç¤ºæŸä¸ªä»»åŠ¡å¤±è´¥ä¸å½±å“å…¶ä»–ä»»åŠ¡
            all_results = await asyncio.gather(*tasks, return_exceptions=self.parallel_config.fail_strategy == "continue")
        return all_results

    async def stream(self, query: str, context_id: str = None, task_id: str = None):
        """ä¸»å…¥å£æµå¼å‡½æ•°ï¼ˆæ”¯æŒå¹¶è¡Œä¸ä¾èµ–ï¼‰"""
        trace_id = generate_trace_id()
        logger.info(f"[bold magenta]ğŸš€ Workflow Started[/bold magenta] (Query: '{query}')")
        yield self.format_response(f"æ”¶åˆ°ä»»åŠ¡ï¼š{query}ï¼Œæ­£åœ¨åˆ†æ... (trace_id: {trace_id})", is_complete=False)

        try:
            # åˆå§‹åŒ–ä¸Šä¸‹æ–‡å’ŒçŠ¶æ€
            if trace_id in self.global_context_store:
                self.global_context = self.global_context_store[trace_id]
                saved_steps = self.step_status_store[trace_id]
                logger.info(f"ğŸ”„ Resuming workflow from breakpoint (trace_id: {trace_id})")
                yield self.format_response(f"å‘ç°æ–­ç‚¹ï¼Œå°†ä»ä¸Šæ¬¡ä¸­æ–­å¤„ç»§ç»­æ‰§è¡Œ... (trace_id: {trace_id})", is_complete=False)
            else:
                self.global_context = {"user_query": query, "trace_id": trace_id}
                saved_steps = {}

            # Phase 1: è§£æä¸DAGåˆå§‹åŒ–
            plan = await self.parser.parse(query, context_id, task_id)
            self.global_context["trace_id"] = plan.trace_id
            yield self.format_response(f"è®¡åˆ’ç”Ÿæˆå®Œæ¯•ï¼Œå…± {len(plan.steps)} ä¸ªæ­¥éª¤ã€‚", is_complete=False)

            # Phase 2: åŒ¹é…Agent
            plan = await self.matcher.match_agents(plan)
            yield self.format_response("èµ„æºè°ƒåº¦å®Œæ¯•ï¼ŒAgent åŒ¹é…å®Œæˆã€‚", is_complete=False)

            # åˆå¹¶ä¿å­˜çš„æ­¥éª¤çŠ¶æ€
            for step_id, saved_step in saved_steps.items():
                if step_id in plan.step_map:
                    plan.step_map[step_id] = saved_step

            # Phase 3: æ‹“æ‰‘æ’åº + å¹¶è¡Œæ‰§è¡Œ
            queue = deque()
            for step in plan.steps:
                if step.in_degree == 0 and step.status == "pending":
                    queue.append(step.step_id)

            logger.info("[bold yellow]âš¡ï¸ Starting Execution Phase[/bold yellow]")

            while queue:
                # å–å‡ºå½“å‰æ‰€æœ‰å…¥åº¦ä¸º0çš„æ­¥éª¤ï¼ˆå¯å¹¶è¡Œæ‰§è¡Œï¼‰
                current_parallel_steps = [plan.step_map[step_id] for step_id in queue]
                queue.clear()

                if not current_parallel_steps:
                    break

                # è¾“å‡ºå¹¶è¡Œæ‰§è¡Œæç¤º
                step_ids = [step.step_id for step in current_parallel_steps]
                step_names = [step.name for step in current_parallel_steps]
                logger.info(f"â–¶ï¸  Executing Batch: Steps {step_ids} ({', '.join(step_names)})")
                yield self.format_response(f"æ­£åœ¨æ‰§è¡Œæ­¥éª¤ï¼š{', '.join(step_names)}...", is_complete=False)

                # å¹¶è¡Œæ‰§è¡Œæ­¥éª¤
                results = await self._execute_parallel_steps(current_parallel_steps, self.global_context, plan.trace_id)

                # å¤„ç†æ‰§è¡Œç»“æœ
                for result in results:
                    if isinstance(result, Exception):
                        # å¤„ç†ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸
                        logger.error(f"âŒ Step execution failed: {result}")
                        yield self.format_response(f"æ­¥éª¤æ‰§è¡Œå¼‚å¸¸ï¼š{str(result)[:100]}...", is_complete=False)
                        continue
                    
                    step, result_str = result # type: ignore
                    # æ›´æ–°ä¸Šä¸‹æ–‡å’ŒçŠ¶æ€
                    self.global_context[f"step_{step.step_id}_output"] = result_str
                    self.step_status_store[plan.trace_id] = self.step_status_store.get(plan.trace_id, {})
                    self.step_status_store[plan.trace_id][step.step_id] = step

                    # è¾“å‡ºæ­¥éª¤ç»“æœ
                    preview = result_str[:100] + "..." if len(str(result_str)) > 100 else result_str
                    status_icon = "âœ…" if step.status == "success" else "âŒ"
                    logger.info(f"  {status_icon} Step {step.step_id} finished: {preview}")
                    yield self.format_response(f"æ­¥éª¤ {step.step_id} {step.status}ã€‚è¾“å‡º: {preview}", is_complete=False)

                    # æ›´æ–°åç»§æ­¥éª¤çš„å…¥åº¦
                    for succ_id in step.successors:
                        succ_step = plan.step_map[succ_id]
                        succ_step.in_degree -= 1
                        # å…¥åº¦ä¸º0åˆ™åŠ å…¥é˜Ÿåˆ—ï¼ˆåç»­å¯æ‰§è¡Œï¼‰
                        if succ_step.in_degree == 0 and succ_step.status == "pending":
                            queue.append(succ_id)

            # æ±‡æ€»æœ€ç»ˆç»“æœå¹¶ä¿å­˜
            final_output = []
            for step in plan.steps:
                if step.status == "success":
                    final_output.append(f"## Step {step.step_id}: {step.name}\n\n{step.result}\n")
            
            full_result_text = "\n".join(final_output)
            saved_path = self._save_result_to_file(query, full_result_text, plan.trace_id, plan)

            # ä¿å­˜ä¸Šä¸‹æ–‡
            self.global_context_store[plan.trace_id] = self.global_context
            logger.info(f"[bold green]ğŸ Workflow Completed Successfully![/bold green] (trace_id: {plan.trace_id})")
            
            completion_msg = f"âœ… æ‰€æœ‰ä»»åŠ¡æ­¥éª¤æ‰§è¡Œå®Œæ¯•ï¼"
            if saved_path:
                completion_msg += f"\nğŸ“„ ç»“æœå·²ä¿å­˜è‡³: {saved_path}"
                
            yield self.format_response(completion_msg, is_complete=True)

        except Exception as e:
            logger.error(f"Workflow crashed (trace_id: {trace_id}): {traceback.format_exc()}")
            yield self.format_response(f"Workflow Critical Error: {e} (trace_id: {trace_id})")
